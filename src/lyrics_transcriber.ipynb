{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fecbf948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating vocals from music with Demucs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals extracted to: C:\\Users\\marcu\\AppData\\Local\\Temp\\tmpb_apc9r3.wav\n",
      "Loading WhisperX model (base.en)...\n",
      "2025-11-16 17:46:40 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n",
      "2025-11-16 17:46:40 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio...\n",
      "Loading alignment model...\n",
      "Loading alignment model...\n",
      "Aligning words...\n",
      "Aligning words...\n",
      "Extracted 22 words from WhisperX\n",
      "0: 'come' - 0.03s to 0.53s\n",
      "1: 'in' - 0.55s to 0.63s\n",
      "2: 'and' - 0.65s to 0.71s\n",
      "3: 'shine' - 0.73s to 1.47s\n",
      "4: 'and' - 1.57s to 1.66s\n",
      "5: 'a' - 1.74s to 1.75s\n",
      "6: 'couple' - 1.81s to 2.14s\n",
      "7: 'of' - 2.16s to 2.20s\n",
      "8: 'different' - 2.22s to 2.46s\n",
      "9: 'colors' - 2.48s to 3.20s\n",
      "10: 'shoot' - 4.26s to 4.48s\n",
      "11: 'a' - 4.56s to 4.58s\n",
      "12: 'cannon' - 4.62s to 5.00s\n",
      "13: 'at' - 5.06s to 5.20s\n",
      "14: 'yourself' - 5.22s to 5.79s\n",
      "15: 'or' - 8.29s to 8.43s\n",
      "16: 'it's' - 8.45s to 8.59s\n",
      "17: 'a' - 8.67s to 8.71s\n",
      "18: 'cradle' - 8.75s to 9.21s\n",
      "19: 'that' - 9.29s to 9.67s\n",
      "20: 'you're' - 9.69s to 10.64s\n",
      "21: 'hollow' - 10.70s to 10.82s\n",
      "\n",
      "Aligned 18 lyric words using ASR duration + short-word bias (bias=0.3).\n",
      "\n",
      "Your lyrics with timings:\n",
      "0: 'conviction' - 0.03s to 0.90s\n",
      "1: 'in' - 0.90s to 1.45s\n",
      "2: 'a' - 1.45s to 1.50s\n",
      "3: 'couple' - 1.50s to 1.89s\n",
      "4: 'different' - 1.89s to 2.25s\n",
      "5: 'colors' - 2.25s to 3.20s\n",
      "6: 'shoot' - 4.26s to 4.46s\n",
      "7: 'a' - 4.46s to 4.47s\n",
      "8: 'cannon' - 4.47s to 4.84s\n",
      "9: 'at' - 4.84s to 4.94s\n",
      "10: 'yourself' - 4.94s to 5.79s\n",
      "11: 'or' - 8.29s to 8.41s\n",
      "12: 'it's' - 8.41s to 8.55s\n",
      "13: 'a' - 8.55s to 8.58s\n",
      "14: 'credo' - 8.58s to 9.11s\n",
      "15: 'that' - 9.11s to 9.50s\n",
      "16: 'you' - 9.50s to 10.37s\n",
      "17: 'holler' - 10.37s to 10.82s\n",
      "Extracted 22 words from WhisperX\n",
      "0: 'come' - 0.03s to 0.53s\n",
      "1: 'in' - 0.55s to 0.63s\n",
      "2: 'and' - 0.65s to 0.71s\n",
      "3: 'shine' - 0.73s to 1.47s\n",
      "4: 'and' - 1.57s to 1.66s\n",
      "5: 'a' - 1.74s to 1.75s\n",
      "6: 'couple' - 1.81s to 2.14s\n",
      "7: 'of' - 2.16s to 2.20s\n",
      "8: 'different' - 2.22s to 2.46s\n",
      "9: 'colors' - 2.48s to 3.20s\n",
      "10: 'shoot' - 4.26s to 4.48s\n",
      "11: 'a' - 4.56s to 4.58s\n",
      "12: 'cannon' - 4.62s to 5.00s\n",
      "13: 'at' - 5.06s to 5.20s\n",
      "14: 'yourself' - 5.22s to 5.79s\n",
      "15: 'or' - 8.29s to 8.43s\n",
      "16: 'it's' - 8.45s to 8.59s\n",
      "17: 'a' - 8.67s to 8.71s\n",
      "18: 'cradle' - 8.75s to 9.21s\n",
      "19: 'that' - 9.29s to 9.67s\n",
      "20: 'you're' - 9.69s to 10.64s\n",
      "21: 'hollow' - 10.70s to 10.82s\n",
      "\n",
      "Aligned 18 lyric words using ASR duration + short-word bias (bias=0.3).\n",
      "\n",
      "Your lyrics with timings:\n",
      "0: 'conviction' - 0.03s to 0.90s\n",
      "1: 'in' - 0.90s to 1.45s\n",
      "2: 'a' - 1.45s to 1.50s\n",
      "3: 'couple' - 1.50s to 1.89s\n",
      "4: 'different' - 1.89s to 2.25s\n",
      "5: 'colors' - 2.25s to 3.20s\n",
      "6: 'shoot' - 4.26s to 4.46s\n",
      "7: 'a' - 4.46s to 4.47s\n",
      "8: 'cannon' - 4.47s to 4.84s\n",
      "9: 'at' - 4.84s to 4.94s\n",
      "10: 'yourself' - 4.94s to 5.79s\n",
      "11: 'or' - 8.29s to 8.41s\n",
      "12: 'it's' - 8.41s to 8.55s\n",
      "13: 'a' - 8.55s to 8.58s\n",
      "14: 'credo' - 8.58s to 9.11s\n",
      "15: 'that' - 9.11s to 9.50s\n",
      "16: 'you' - 9.50s to 10.37s\n",
      "17: 'holler' - 10.37s to 10.82s\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import whisperx\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# Install: pip install demucs\n",
    "import torch\n",
    "import torchaudio\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "song_path = 'conviction.wav'\n",
    "lyrics_text = \"\"\"\n",
    "Conviction in a couple different colors\n",
    "Shoot a cannon at yourself\n",
    "Or it's a credo that you holler\n",
    "\"\"\"\n",
    "\n",
    "# Options\n",
    "use_vocal_separation = True\n",
    "whisperx_model_size = \"base.en\"\n",
    "device = \"cpu\"\n",
    "short_word_bias = 0.3  # 0.0 = no bias (pure ASR durations); higher shrinks short words / extends long words within phrase\n",
    "\n",
    "\n",
    "def extract_vocals(audio_path):\n",
    "    \"\"\"Separate vocals using Demucs (no DLL issues on Windows)\"\"\"\n",
    "    print(\"Separating vocals from music with Demucs...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = get_model('htdemucs')\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    \n",
    "    # Load audio\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Apply separation\n",
    "    with torch.no_grad():\n",
    "        sources = apply_model(model, wav[None], device='cpu')[0]\n",
    "    \n",
    "    # Extract vocals (index 3)\n",
    "    vocals = sources[3]\n",
    "    \n",
    "    # Save to temp file - FIXED: Use NamedTemporaryFile instead of mktemp\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        temp_vocals = tmp.name\n",
    "    \n",
    "    torchaudio.save(temp_vocals, vocals, sr)\n",
    "\n",
    "    \n",
    "    print(f\"Vocals extracted to: {temp_vocals}\")\n",
    "    return temp_vocals\n",
    "\n",
    "\n",
    "def _tokenize_words(text: str):\n",
    "    # keep letters and apostrophes for words like it's, you're\n",
    "    return re.findall(r\"[A-Za-z']+\", text.lower())\n",
    "\n",
    "\n",
    "def _split_lyrics_into_phrases(lyrics_text: str):\n",
    "    # Split by lines; drop empties\n",
    "    lines = [ln.strip() for ln in lyrics_text.splitlines()]\n",
    "    lines = [ln for ln in lines if ln]\n",
    "    phrases = []\n",
    "    for ln in lines:\n",
    "        words = _tokenize_words(ln)\n",
    "        if words:\n",
    "            phrases.append({\"text\": ln, \"words\": words})\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def _group_asr_into_phrases(whisperx_words, gap_threshold: float = 0.2):\n",
    "    # Group contiguous ASR words separated by gaps > threshold\n",
    "    phrases = []\n",
    "    if not whisperx_words:\n",
    "        return phrases\n",
    "    current = {\"words\": [], \"start\": whisperx_words[0]['start'], \"end\": whisperx_words[0]['end']}\n",
    "    for w in whisperx_words:\n",
    "        if not current[\"words\"]:\n",
    "            current[\"words\"].append(w)\n",
    "            current[\"start\"] = w[\"start\"]\n",
    "            current[\"end\"] = w[\"end\"]\n",
    "            continue\n",
    "        gap = w[\"start\"] - current[\"words\"][-1][\"end\"]\n",
    "        if gap > gap_threshold:\n",
    "            phrases.append(current)\n",
    "            current = {\"words\": [w], \"start\": w[\"start\"], \"end\": w[\"end\"]}\n",
    "        else:\n",
    "            current[\"words\"].append(w)\n",
    "            current[\"end\"] = w[\"end\"]\n",
    "    if current[\"words\"]:\n",
    "        phrases.append(current)\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z']+\", \"\", s.lower())\n",
    "\n",
    "\n",
    "def _sim(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def _segment_asr_groups(lyric_words, asr_words_in_phrase):\n",
    "    \"\"\"\n",
    "    Greedily group contiguous ASR tokens to each lyric word.\n",
    "    Returns list of dicts: {word, base_start, base_end, len_norm}\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    awords = asr_words_in_phrase\n",
    "    j = 0\n",
    "    for lw in lyric_words:\n",
    "        lw_norm = _norm(lw)\n",
    "        if j >= len(awords):\n",
    "            # no ASR left; estimate tiny duration after previous\n",
    "            if groups:\n",
    "                base_start = groups[-1]['base_end']\n",
    "            else:\n",
    "                base_start = awords[-1]['end'] if awords else 0.0\n",
    "            base_end = base_start + 0.3\n",
    "            groups.append({\"word\": lw, \"base_start\": base_start, \"base_end\": base_end, \"len_norm\": len(lw_norm) or 1})\n",
    "            continue\n",
    "        g_start = j\n",
    "        g_end = j\n",
    "        current_concat = _norm(awords[g_end]['word'])\n",
    "        best_sim = _sim(current_concat, lw_norm) if lw_norm else 1.0\n",
    "        best_end = g_end\n",
    "        while g_end + 1 < len(awords):\n",
    "            trial = current_concat + _norm(awords[g_end + 1]['word'])\n",
    "            trial_sim = _sim(trial, lw_norm) if lw_norm else 1.0\n",
    "            len_ratio = len(trial) / max(1, len(lw_norm))\n",
    "            if trial_sim >= best_sim or (best_sim < 0.6 and len_ratio < 1.3):\n",
    "                g_end += 1\n",
    "                current_concat = trial\n",
    "                best_sim = trial_sim\n",
    "                best_end = g_end\n",
    "            else:\n",
    "                break\n",
    "        base_start = float(awords[g_start]['start'])\n",
    "        base_end = float(awords[best_end]['end'])\n",
    "        groups.append({\"word\": lw, \"base_start\": base_start, \"base_end\": base_end, \"len_norm\": len(lw_norm) or 1})\n",
    "        j = best_end + 1\n",
    "    # If leftover ASR tokens remain, extend last group's end to include them\n",
    "    if j < len(awords) and groups:\n",
    "        groups[-1]['base_end'] = float(awords[-1]['end'])\n",
    "    return groups\n",
    "\n",
    "\n",
    "def _apply_duration_bias(groups, phrase_start, phrase_end, bias: float):\n",
    "    \"\"\"\n",
    "    Adjust durations per lyric word around their ASR-based base durations using a\n",
    "    length-based bias. Preserves total phrase duration and ordering.\n",
    "    bias in [0,1]: 0 = no change; higher = stronger long>short emphasis.\n",
    "    \"\"\"\n",
    "    if not groups:\n",
    "        return []\n",
    "    # Base durations from ASR grouping\n",
    "    base_durs = [max(0.01, g['base_end'] - g['base_start']) for g in groups]\n",
    "    total_base = sum(base_durs)\n",
    "    if total_base <= 1e-6:\n",
    "        # fallback: equal tiny splits\n",
    "        step = (phrase_end - phrase_start) / len(groups)\n",
    "        t = phrase_start\n",
    "        out = []\n",
    "        for g in groups:\n",
    "            out.append({\"word\": g['word'], \"start\": t, \"end\": t + step})\n",
    "            t += step\n",
    "        out[-1]['end'] = phrase_end\n",
    "        return out\n",
    "\n",
    "    # Compute weights based on word length relative to mean\n",
    "    mean_len = max(1.0, sum(g['len_norm'] for g in groups) / len(groups))\n",
    "    weights = []\n",
    "    for g in groups:\n",
    "        ln = max(1.0, float(g['len_norm']))\n",
    "        rel = ln / mean_len\n",
    "        # weight factor: 1 blended toward rel**gamma by bias\n",
    "        gamma = 1.0 + 1.0 * bias\n",
    "        w = (1 - bias) + bias * (rel ** gamma)\n",
    "        weights.append(w)\n",
    "\n",
    "    # Apply weights to base durations, then renormalize to keep total the same\n",
    "    adjusted = [d * w for d, w in zip(base_durs, weights)]\n",
    "    sum_adj = sum(adjusted)\n",
    "    if sum_adj <= 1e-6:\n",
    "        adjusted = base_durs[:]  # fallback\n",
    "        sum_adj = total_base\n",
    "    scale = total_base / sum_adj\n",
    "    adjusted = [a * scale for a in adjusted]\n",
    "\n",
    "    # Lay out sequentially within the phrase window, preserving total window\n",
    "    # Align the start to min(base_starts, phrase_start) for stability\n",
    "    t = max(phrase_start, min(g['base_start'] for g in groups))\n",
    "    # If the first ASR starts after phrase_start, use that; otherwise phrase_start\n",
    "    t = phrase_start\n",
    "    out = []\n",
    "    for g, dur in zip(groups, adjusted):\n",
    "        start = t\n",
    "        end = min(phrase_end, start + dur)\n",
    "        out.append({\"word\": g['word'], \"start\": start, \"end\": end})\n",
    "        t = end\n",
    "    # ensure phrase end exact\n",
    "    if out:\n",
    "        out[-1]['end'] = phrase_end\n",
    "    return out\n",
    "\n",
    "\n",
    "def _segment_asr_to_lyrics_with_bias(lyrics_phrases, asr_phrases, bias: float):\n",
    "    timings = []\n",
    "    pairs = min(len(lyrics_phrases), len(asr_phrases))\n",
    "    for i in range(pairs):\n",
    "        lp = lyrics_phrases[i]\n",
    "        ap = asr_phrases[i]\n",
    "        lwords = lp[\"words\"]\n",
    "        awords = ap[\"words\"]\n",
    "        s, e = float(ap['start']), float(ap['end'])\n",
    "        if not lwords or e <= s:\n",
    "            continue\n",
    "        groups = _segment_asr_groups(lwords, awords)\n",
    "        timings.extend(_apply_duration_bias(groups, s, e, bias))\n",
    "    # handle extra lyric phrases if any\n",
    "    if len(lyrics_phrases) > pairs:\n",
    "        t = timings[-1]['end'] if timings else (asr_phrases[-1]['end'] if asr_phrases else 0.0)\n",
    "        for i in range(pairs, len(lyrics_phrases)):\n",
    "            for w in lyrics_phrases[i]['words']:\n",
    "                end = t + 0.3\n",
    "                timings.append({\"word\": w, \"start\": t, \"end\": end})\n",
    "                t = end\n",
    "    return timings\n",
    "\n",
    "\n",
    "def align_lyrics_to_audio(audio_path, lyrics_text, use_vocal_separation=True):\n",
    "    \"\"\"\n",
    "    Align YOUR lyrics to the audio timing using WhisperX: first group ASR tokens to lyric\n",
    "    words (preserving ASR-based durations), then apply a short-word bias that slightly\n",
    "    reduces durations of short/function words and increases durations of longer words,\n",
    "    while preserving each phrase's total duration.\n",
    "\n",
    "    Returns a list of word timings for YOUR lyrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract vocals if enabled\n",
    "    if use_vocal_separation:\n",
    "        audio_to_use = extract_vocals(audio_path)\n",
    "    else:\n",
    "        audio_to_use = audio_path\n",
    "    \n",
    "    # Step 2: Load WhisperX model\n",
    "    print(f\"Loading WhisperX model ({whisperx_model_size})...\")\n",
    "    model = whisperx.load_model(whisperx_model_size, device, compute_type=\"float32\")\n",
    "    \n",
    "    # Step 3: Load audio\n",
    "    audio = whisperx.load_audio(audio_to_use)\n",
    "    \n",
    "    # Step 4: Transcribe to get initial segments (disable VAD for music)\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio, batch_size=16, language=\"en\")\n",
    "    \n",
    "    # Step 5: Load alignment model for word-level timestamps\n",
    "    print(\"Loading alignment model...\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "    \n",
    "    # Step 6: Align to get precise word timings\n",
    "    print(\"Aligning words...\")\n",
    "    result_aligned = whisperx.align(\n",
    "        result[\"segments\"], \n",
    "        model_a, \n",
    "        metadata, \n",
    "        audio, \n",
    "        device,\n",
    "        return_char_alignments=False\n",
    "    )\n",
    "    \n",
    "    # Step 7: Extract word timings from WhisperX\n",
    "    whisperx_words = []\n",
    "    for segment in result_aligned.get(\"segments\", []):\n",
    "        for word_info in segment.get(\"words\", []):\n",
    "            if word_info.get('word') is None:\n",
    "                continue\n",
    "            whisperx_words.append({\n",
    "                'word': word_info['word'].strip().lower(),\n",
    "                'start': float(word_info['start']),\n",
    "                'end': float(word_info['end'])\n",
    "            })\n",
    "    print(f\"Extracted {len(whisperx_words)} words from WhisperX\")\n",
    "    # Optional debug (first 50)\n",
    "    for idx, w in enumerate(whisperx_words[:50]):\n",
    "        print(f\"{idx}: '{w['word']}' - {w['start']:.2f}s to {w['end']:.2f}s\")\n",
    "\n",
    "    # Step 8: Build lyric phrases (lines)\n",
    "    lyric_phrases = _split_lyrics_into_phrases(lyrics_text)\n",
    "    if not lyric_phrases:\n",
    "        raise ValueError(\"No lyric phrases found. Provide non-empty lyrics_text.\")\n",
    "\n",
    "    # Step 9: Group ASR words into phrases by time gaps\n",
    "    asr_phrases = _group_asr_into_phrases(whisperx_words, gap_threshold=0.2)\n",
    "    if not asr_phrases and whisperx_words:\n",
    "        asr_phrases = [{\"words\": whisperx_words, \"start\": whisperx_words[0]['start'], \"end\": whisperx_words[-1]['end'] }]\n",
    "\n",
    "    # Step 10: Segment & bias-adjust using ASR base durations\n",
    "    word_timings = _segment_asr_to_lyrics_with_bias(lyric_phrases, asr_phrases, short_word_bias)\n",
    "\n",
    "    print(f\"\\nAligned {len(word_timings)} lyric words using ASR duration + short-word bias (bias={short_word_bias}).\")\n",
    "    return word_timings\n",
    "\n",
    "\n",
    "# Run the alignment\n",
    "word_timings = align_lyrics_to_audio(song_path, lyrics_text, use_vocal_separation)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nYour lyrics with timings:\")\n",
    "for i, word_data in enumerate(word_timings):\n",
    "    print(f\"{i}: '{word_data['word']}' - {word_data['start']:.2f}s to {word_data['end']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3805101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote silent video: out\\lyrics_black_temp.mp4\n",
      "Running: ffmpeg -y -i out\\lyrics_black_temp.mp4 -i conviction.wav -c:v copy -c:a aac -shortest out\\lyrics_black_with_audio.mp4\n",
      "Wrote final video with audio: out\\lyrics_black_with_audio.mp4\n",
      "Wrote final video with audio: out\\lyrics_black_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Inputs\n",
    "output_dir = Path(\"out\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_fps = 30\n",
    "width, height = 1920, 1080\n",
    "\n",
    "# Font configuration\n",
    "# If font_path points to a valid .ttf/.otf, we will use PIL TrueType at font_size_px.\n",
    "# Otherwise we fall back to OpenCV Hershey using hershey_scale.\n",
    "font_path = r\"KGRedHands.ttf\"  # set to None or to a valid TTF path\n",
    "font_size_px = 120  # applies when using TrueType font via PIL\n",
    "hershey_scale = 2.5  # applies when using OpenCV Hershey fonts\n",
    "\n",
    "font_color = (255, 255, 255)\n",
    "font_thickness = 3\n",
    "stroke_color = (0, 0, 0)\n",
    "stroke_thickness = 6\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "# Source audio and timings from previous cells\n",
    "source_audio = song_path  # from earlier cell\n",
    "word_timings_in = word_timings  # from earlier cell (list of dicts: word,start,end)\n",
    "\n",
    "# Video temp paths\n",
    "temp_video_path = output_dir / \"lyrics_black_temp.mp4\"\n",
    "final_video_path = output_dir / \"lyrics_black_with_audio.mp4\"\n",
    "\n",
    "# Prepare VideoWriter (H264 via mp4v/avc1 depends on platform; fallback to MJPG)\n",
    "fourcc = cv2.VideoWriter.fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(temp_video_path), fourcc, video_fps, (width, height))\n",
    "\n",
    "# Decide which font pipeline to use\n",
    "use_pil_font = bool(font_path) and Path(font_path).exists()\n",
    "if not use_pil_font and font_path:\n",
    "    print(f\"Warning: font_path not found: {font_path}. Falling back to OpenCV Hershey font.\")\n",
    "\n",
    "# Helper to draw centered text with optional stroke\n",
    "def draw_centered_text(img, text):\n",
    "    if not use_pil_font:\n",
    "        # OpenCV Hershey path (uses hershey_scale)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_size, _ = cv2.getTextSize(text, font, hershey_scale, font_thickness)\n",
    "        tw, th = text_size\n",
    "        x = (width - tw) // 2\n",
    "        y = (height + th) // 2\n",
    "        if stroke_thickness > 0:\n",
    "            cv2.putText(img, text, (x, y), font, hershey_scale, stroke_color, stroke_thickness, line_type)\n",
    "        cv2.putText(img, text, (x, y), font, hershey_scale, font_color, font_thickness, line_type)\n",
    "    else:\n",
    "        # PIL TrueType path (uses font_size_px)\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        pil_img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        try:\n",
    "            fnt = ImageFont.truetype(font_path, font_size_px)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load TrueType font '{font_path}': {e}. Falling back to Hershey.\")\n",
    "            # Fallback to Hershey immediately\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text_size, _ = cv2.getTextSize(text, font, hershey_scale, font_thickness)\n",
    "            tw, th = text_size\n",
    "            x = (width - tw) // 2\n",
    "            y = (height + th) // 2\n",
    "            if stroke_thickness > 0:\n",
    "                cv2.putText(img, text, (x, y), font, hershey_scale, stroke_color, stroke_thickness, line_type)\n",
    "            cv2.putText(img, text, (x, y), font, hershey_scale, font_color, font_thickness, line_type)\n",
    "            return\n",
    "        # Measure text\n",
    "        bbox = draw.textbbox((0, 0), text, font=fnt)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x = (width - tw) // 2\n",
    "        y = (height - th) // 2\n",
    "        # crude stroke by drawing offset shadows\n",
    "        if stroke_thickness > 0:\n",
    "            r = max(1, stroke_thickness // 2)\n",
    "            for dx, dy in [(-r,0),(r,0),(0,-r),(0,r),(-r,-r),(-r,r),(r,-r),(r,r)]:\n",
    "                draw.text((x+dx, y+dy), text, font=fnt, fill=stroke_color)\n",
    "        draw.text((x, y), text, font=fnt, fill=font_color)\n",
    "        img[:] = np.array(pil_img)\n",
    "\n",
    "# Normalize/clean timings\n",
    "def clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "valid_timings = []\n",
    "for w in word_timings_in:\n",
    "    try:\n",
    "        s = float(w.get('start', 0.0))\n",
    "        e = float(w.get('end', s + 0.25))\n",
    "        if e <= s:\n",
    "            e = s + 0.25\n",
    "        valid_timings.append({'word': str(w.get('word','')), 'start': s, 'end': e})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not valid_timings:\n",
    "    raise RuntimeError(\n",
    "        \"word_timings is empty. Please ensure you have:\\n\"\n",
    "        \"1. Run the word alignment cell that populates 'word_timings'\\n\"\n",
    "        \"2. Verified that the alignment completed successfully\\n\"\n",
    "        f\"Current word_timings length: {len(word_timings_in)}\"\n",
    "    )\n",
    "\n",
    "total_duration = valid_timings[-1]['end']\n",
    "\n",
    "def frame_for_time(t):\n",
    "    # Generate a single frame for time t\n",
    "    # Find the active word whose [start,end) contains t\n",
    "    active = None\n",
    "    for w in valid_timings:\n",
    "        if w['start'] <= t < w['end']:\n",
    "            active = w\n",
    "            break\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    if active:\n",
    "        draw_centered_text(img, active['word'])\n",
    "    return img\n",
    "\n",
    "# Render frames\n",
    "num_frames = int(math.ceil(total_duration * video_fps))\n",
    "for i in range(num_frames):\n",
    "    t = i / video_fps\n",
    "    frame = frame_for_time(t)\n",
    "    writer.write(frame)\n",
    "\n",
    "writer.release()\n",
    "print(f\"Wrote silent video: {temp_video_path}\")\n",
    "\n",
    "# Mux audio using ffmpeg\n",
    "# - Copy video stream, re-encode audio to AAC for compatibility\n",
    "# - Shorten to the shorter of video/audio so they end together\n",
    "ffmpeg_cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",\n",
    "    \"-i\", str(temp_video_path),\n",
    "    \"-i\", str(source_audio),\n",
    "    \"-c:v\", \"copy\",\n",
    "    \"-c:a\", \"aac\",\n",
    "    \"-shortest\",\n",
    "    str(final_video_path)\n",
    "]\n",
    "print(\"Running:\", \" \".join(ffmpeg_cmd))\n",
    "try:\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "    print(f\"Wrote final video with audio: {final_video_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ffmpeg not found on PATH. Please install FFmpeg or add it to PATH.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-whisperx-py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
