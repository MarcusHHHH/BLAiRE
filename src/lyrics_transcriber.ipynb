{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecbf948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating vocals from music with Demucs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals extracted to: C:\\Users\\marcu\\AppData\\Local\\Temp\\tmp55wag2t3.wav\n",
      "Loading WhisperX model (base.en)...\n",
      "2025-11-16 17:08:20 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n",
      "2025-11-16 17:08:20 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documents\\CodeStuff\\BLAiRE\\.venv-whisperx-py313\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio...\n",
      "Loading alignment model...\n",
      "Loading alignment model...\n",
      "Aligning words...\n",
      "Aligning words...\n",
      "Extracted 22 words from WhisperX\n",
      "0: 'come' - 0.03s to 0.53s\n",
      "1: 'in' - 0.55s to 0.63s\n",
      "2: 'and' - 0.65s to 0.71s\n",
      "3: 'shine' - 0.73s to 1.48s\n",
      "4: 'in' - 1.58s to 1.66s\n",
      "5: 'a' - 1.74s to 1.78s\n",
      "6: 'couple' - 1.82s to 2.20s\n",
      "7: 'of' - 2.22s to 2.26s\n",
      "8: 'different' - 2.28s to 2.46s\n",
      "9: 'colors' - 2.48s to 3.20s\n",
      "10: 'shoot' - 4.26s to 4.49s\n",
      "11: 'a' - 4.57s to 4.58s\n",
      "12: 'cannon' - 4.62s to 5.01s\n",
      "13: 'at' - 5.07s to 5.21s\n",
      "14: 'yourself' - 5.23s to 5.79s\n",
      "15: 'or' - 8.30s to 8.44s\n",
      "16: 'it's' - 8.46s to 8.60s\n",
      "17: 'a' - 8.68s to 8.72s\n",
      "18: 'cradle' - 8.76s to 9.24s\n",
      "19: 'that' - 9.30s to 9.66s\n",
      "20: 'you're' - 9.68s to 10.64s\n",
      "21: 'hollow' - 10.77s to 10.88s\n",
      "\n",
      "Aligned 18 lyric words using phrase segmentation.\n",
      "\n",
      "Your lyrics with timings:\n",
      "0: 'conviction' - 0.03s to 0.71s\n",
      "1: 'in' - 0.73s to 1.48s\n",
      "2: 'a' - 1.58s to 1.78s\n",
      "3: 'couple' - 1.82s to 2.20s\n",
      "4: 'different' - 2.22s to 2.46s\n",
      "5: 'colors' - 2.48s to 3.20s\n",
      "6: 'shoot' - 4.26s to 4.49s\n",
      "7: 'a' - 4.57s to 4.58s\n",
      "8: 'cannon' - 4.62s to 5.01s\n",
      "9: 'at' - 5.07s to 5.21s\n",
      "10: 'yourself' - 5.23s to 5.79s\n",
      "11: 'or' - 8.30s to 8.44s\n",
      "12: 'it's' - 8.46s to 8.60s\n",
      "13: 'a' - 8.68s to 8.72s\n",
      "14: 'credo' - 8.76s to 9.24s\n",
      "15: 'that' - 9.30s to 9.66s\n",
      "16: 'you' - 9.68s to 10.64s\n",
      "17: 'holler' - 10.77s to 10.88s\n",
      "Extracted 22 words from WhisperX\n",
      "0: 'come' - 0.03s to 0.53s\n",
      "1: 'in' - 0.55s to 0.63s\n",
      "2: 'and' - 0.65s to 0.71s\n",
      "3: 'shine' - 0.73s to 1.48s\n",
      "4: 'in' - 1.58s to 1.66s\n",
      "5: 'a' - 1.74s to 1.78s\n",
      "6: 'couple' - 1.82s to 2.20s\n",
      "7: 'of' - 2.22s to 2.26s\n",
      "8: 'different' - 2.28s to 2.46s\n",
      "9: 'colors' - 2.48s to 3.20s\n",
      "10: 'shoot' - 4.26s to 4.49s\n",
      "11: 'a' - 4.57s to 4.58s\n",
      "12: 'cannon' - 4.62s to 5.01s\n",
      "13: 'at' - 5.07s to 5.21s\n",
      "14: 'yourself' - 5.23s to 5.79s\n",
      "15: 'or' - 8.30s to 8.44s\n",
      "16: 'it's' - 8.46s to 8.60s\n",
      "17: 'a' - 8.68s to 8.72s\n",
      "18: 'cradle' - 8.76s to 9.24s\n",
      "19: 'that' - 9.30s to 9.66s\n",
      "20: 'you're' - 9.68s to 10.64s\n",
      "21: 'hollow' - 10.77s to 10.88s\n",
      "\n",
      "Aligned 18 lyric words using phrase segmentation.\n",
      "\n",
      "Your lyrics with timings:\n",
      "0: 'conviction' - 0.03s to 0.71s\n",
      "1: 'in' - 0.73s to 1.48s\n",
      "2: 'a' - 1.58s to 1.78s\n",
      "3: 'couple' - 1.82s to 2.20s\n",
      "4: 'different' - 2.22s to 2.46s\n",
      "5: 'colors' - 2.48s to 3.20s\n",
      "6: 'shoot' - 4.26s to 4.49s\n",
      "7: 'a' - 4.57s to 4.58s\n",
      "8: 'cannon' - 4.62s to 5.01s\n",
      "9: 'at' - 5.07s to 5.21s\n",
      "10: 'yourself' - 5.23s to 5.79s\n",
      "11: 'or' - 8.30s to 8.44s\n",
      "12: 'it's' - 8.46s to 8.60s\n",
      "13: 'a' - 8.68s to 8.72s\n",
      "14: 'credo' - 8.76s to 9.24s\n",
      "15: 'that' - 9.30s to 9.66s\n",
      "16: 'you' - 9.68s to 10.64s\n",
      "17: 'holler' - 10.77s to 10.88s\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import whisperx\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# Install: pip install demucs\n",
    "import torch\n",
    "import torchaudio\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "song_path = 'conviction.wav'\n",
    "lyrics_text = \"\"\"\n",
    "Conviction in a couple different colors\n",
    "Shoot a cannon at yourself\n",
    "Or it's a credo that you holler\n",
    "\"\"\"\n",
    "\n",
    "# Options\n",
    "use_vocal_separation = True\n",
    "whisperx_model_size = \"base.en\"\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def extract_vocals(audio_path):\n",
    "    \"\"\"Separate vocals using Demucs (no DLL issues on Windows)\"\"\"\n",
    "    print(\"Separating vocals from music with Demucs...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = get_model('htdemucs')\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    \n",
    "    # Load audio\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Apply separation\n",
    "    with torch.no_grad():\n",
    "        sources = apply_model(model, wav[None], device='cpu')[0]\n",
    "    \n",
    "    # Extract vocals (index 3)\n",
    "    vocals = sources[3]\n",
    "    \n",
    "    # Save to temp file - FIXED: Use NamedTemporaryFile instead of mktemp\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        temp_vocals = tmp.name\n",
    "    \n",
    "    torchaudio.save(temp_vocals, vocals, sr)\n",
    "\n",
    "    \n",
    "    print(f\"Vocals extracted to: {temp_vocals}\")\n",
    "    return temp_vocals\n",
    "\n",
    "\n",
    "def _tokenize_words(text: str):\n",
    "    # keep letters and apostrophes for words like it's, you're\n",
    "    return re.findall(r\"[A-Za-z']+\", text.lower())\n",
    "\n",
    "\n",
    "def _split_lyrics_into_phrases(lyrics_text: str):\n",
    "    # Split by lines; drop empties\n",
    "    lines = [ln.strip() for ln in lyrics_text.splitlines()]\n",
    "    lines = [ln for ln in lines if ln]\n",
    "    phrases = []\n",
    "    for ln in lines:\n",
    "        words = _tokenize_words(ln)\n",
    "        if words:\n",
    "            phrases.append({\"text\": ln, \"words\": words})\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def _group_asr_into_phrases(whisperx_words, gap_threshold: float = 0.9):\n",
    "    # Group contiguous ASR words separated by gaps > threshold\n",
    "    phrases = []\n",
    "    if not whisperx_words:\n",
    "        return phrases\n",
    "    current = {\"words\": [], \"start\": whisperx_words[0]['start'], \"end\": whisperx_words[0]['end']}\n",
    "    for w in whisperx_words:\n",
    "        if not current[\"words\"]:\n",
    "            current[\"words\"].append(w)\n",
    "            current[\"start\"] = w[\"start\"]\n",
    "            current[\"end\"] = w[\"end\"]\n",
    "            continue\n",
    "        gap = w[\"start\"] - current[\"words\"][-1][\"end\"]\n",
    "        if gap > gap_threshold:\n",
    "            phrases.append(current)\n",
    "            current = {\"words\": [w], \"start\": w[\"start\"], \"end\": w[\"end\"]}\n",
    "        else:\n",
    "            current[\"words\"].append(w)\n",
    "            current[\"end\"] = w[\"end\"]\n",
    "    if current[\"words\"]:\n",
    "        phrases.append(current)\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z']+\", \"\", s.lower())\n",
    "\n",
    "\n",
    "def _sim(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def _segment_asr_to_lyrics(lyrics_phrases, asr_phrases):\n",
    "    \"\"\"\n",
    "    For each lyric phrase (line) and the corresponding ASR phrase,\n",
    "    segment the ASR words into contiguous groups mapped to each lyric word.\n",
    "    Each lyric word receives start=min(group), end=max(group) of its ASR group.\n",
    "    If ASR runs out, estimate timings for remaining lyric words.\n",
    "    \"\"\"\n",
    "    timings = []\n",
    "    pairs = min(len(lyrics_phrases), len(asr_phrases))\n",
    "    for i in range(pairs):\n",
    "        lp = lyrics_phrases[i]\n",
    "        ap = asr_phrases[i]\n",
    "        lwords = lp[\"words\"]\n",
    "        awords = ap[\"words\"]\n",
    "        j = 0  # index into ASR words\n",
    "        for lw in lwords:\n",
    "            lw_norm = _norm(lw)\n",
    "            if j >= len(awords):\n",
    "                # No ASR left; estimate 0.4s per word after last timing\n",
    "                start_est = timings[-1][\"end\"] if timings else (ap[\"end\"])  # after phrase\n",
    "                end_est = start_est + 0.4\n",
    "                timings.append({\"word\": lw, \"start\": start_est, \"end\": end_est})\n",
    "                continue\n",
    "            # Start group with at least one ASR word\n",
    "            g_start = j\n",
    "            g_end = j\n",
    "            current_concat = _norm(awords[g_end]['word'])\n",
    "            best_sim = _sim(current_concat, lw_norm)\n",
    "            best_end = g_end\n",
    "            # Try to extend group greedily while similarity improves or modestly decreases but length is small\n",
    "            while g_end + 1 < len(awords):\n",
    "                trial = current_concat + _norm(awords[g_end + 1]['word'])\n",
    "                trial_sim = _sim(trial, lw_norm)\n",
    "                # Heuristics: prefer improvement; allow small decrease if current length is far from target\n",
    "                len_ratio = len(trial) / max(1, len(lw_norm))\n",
    "                if trial_sim >= best_sim or (best_sim < 0.6 and len_ratio < 1.3):\n",
    "                    g_end += 1\n",
    "                    current_concat = trial\n",
    "                    best_sim = trial_sim\n",
    "                    best_end = g_end\n",
    "                else:\n",
    "                    break\n",
    "            # Assign timing from first to last ASR in group\n",
    "            start_t = float(awords[g_start]['start'])\n",
    "            end_t = float(awords[best_end]['end'])\n",
    "            timings.append({\"word\": lw, \"start\": start_t, \"end\": end_t})\n",
    "            j = best_end + 1\n",
    "        # If ASR words remain in this phrase and lyric words exhausted, extend last lyric word to include leftovers\n",
    "        if j < len(awords) and timings:\n",
    "            timings[-1][\"end\"] = float(awords[-1]['end'])\n",
    "    # Handle extra lyric phrases (no matching ASR phrase)\n",
    "    if len(lyrics_phrases) > pairs:\n",
    "        t = timings[-1]['end'] if timings else 0.0\n",
    "        for i in range(pairs, len(lyrics_phrases)):\n",
    "            for w in lyrics_phrases[i][\"words\"]:\n",
    "                timings.append({\"word\": w, \"start\": t, \"end\": t + 0.4})\n",
    "                t += 0.4\n",
    "    return timings\n",
    "\n",
    "\n",
    "def align_lyrics_to_audio(audio_path, lyrics_text, use_vocal_separation=True):\n",
    "    \"\"\"\n",
    "    Align YOUR lyrics to the audio timing using WhisperX by mapping entire phrases (lines)\n",
    "    to contiguous ASR word chunks, then segmenting those chunks to lyric words using a\n",
    "    greedy similarity-based grouping (combines multiple ASR tokens into one lyric token\n",
    "    when needed; e.g., ASR \"come in and shine\" → lyric \"conviction\").\n",
    "\n",
    "    Returns a list of word timings for YOUR lyrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract vocals if enabled\n",
    "    if use_vocal_separation:\n",
    "        audio_to_use = extract_vocals(audio_path)\n",
    "    else:\n",
    "        audio_to_use = audio_path\n",
    "    \n",
    "    # Step 2: Load WhisperX model\n",
    "    print(f\"Loading WhisperX model ({whisperx_model_size})...\")\n",
    "    model = whisperx.load_model(whisperx_model_size, device, compute_type=\"float32\")\n",
    "    \n",
    "    # Step 3: Load audio\n",
    "    audio = whisperx.load_audio(audio_to_use)\n",
    "    \n",
    "    # Step 4: Transcribe to get initial segments (disable VAD for music)\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio, batch_size=16, language=\"en\")\n",
    "    \n",
    "    # Step 5: Load alignment model for word-level timestamps\n",
    "    print(\"Loading alignment model...\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "    \n",
    "    # Step 6: Align to get precise word timings\n",
    "    print(\"Aligning words...\")\n",
    "    result_aligned = whisperx.align(\n",
    "        result[\"segments\"], \n",
    "        model_a, \n",
    "        metadata, \n",
    "        audio, \n",
    "        device,\n",
    "        return_char_alignments=False\n",
    "    )\n",
    "    \n",
    "    # Step 7: Extract word timings from WhisperX\n",
    "    whisperx_words = []\n",
    "    for segment in result_aligned.get(\"segments\", []):\n",
    "        for word_info in segment.get(\"words\", []):\n",
    "            if word_info.get('word') is None:\n",
    "                continue\n",
    "            whisperx_words.append({\n",
    "                'word': word_info['word'].strip().lower(),\n",
    "                'start': float(word_info['start']),\n",
    "                'end': float(word_info['end'])\n",
    "            })\n",
    "    print(f\"Extracted {len(whisperx_words)} words from WhisperX\")\n",
    "    # Optional debug (first 50)\n",
    "    for idx, w in enumerate(whisperx_words[:50]):\n",
    "        print(f\"{idx}: '{w['word']}' - {w['start']:.2f}s to {w['end']:.2f}s\")\n",
    "\n",
    "    # Step 8: Build lyric phrases (lines)\n",
    "    lyric_phrases = _split_lyrics_into_phrases(lyrics_text)\n",
    "    if not lyric_phrases:\n",
    "        raise ValueError(\"No lyric phrases found. Provide non-empty lyrics_text.\")\n",
    "\n",
    "    # Step 9: Group ASR words into phrases by time gaps\n",
    "    asr_phrases = _group_asr_into_phrases(whisperx_words, gap_threshold=0.6)\n",
    "    if not asr_phrases and whisperx_words:\n",
    "        asr_phrases = [{\"words\": whisperx_words, \"start\": whisperx_words[0]['start'], \"end\": whisperx_words[-1]['end'] }]\n",
    "\n",
    "    # Step 10: Segment ASR → lyric words\n",
    "    word_timings = _segment_asr_to_lyrics(lyric_phrases, asr_phrases)\n",
    "\n",
    "    print(f\"\\nAligned {len(word_timings)} lyric words using phrase segmentation.\")\n",
    "    return word_timings\n",
    "\n",
    "\n",
    "# Run the alignment\n",
    "word_timings = align_lyrics_to_audio(song_path, lyrics_text, use_vocal_separation)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nYour lyrics with timings:\")\n",
    "for i, word_data in enumerate(word_timings):\n",
    "    print(f\"{i}: '{word_data['word']}' - {word_data['start']:.2f}s to {word_data['end']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3805101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote silent video: out\\lyrics_black_temp.mp4\n",
      "Running: ffmpeg -y -i out\\lyrics_black_temp.mp4 -i conviction.wav -c:v copy -c:a aac -shortest out\\lyrics_black_with_audio.mp4\n",
      "Wrote final video with audio: out\\lyrics_black_with_audio.mp4\n",
      "Wrote final video with audio: out\\lyrics_black_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Inputs\n",
    "output_dir = Path(\"out\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_fps = 30\n",
    "width, height = 1920, 1080\n",
    "\n",
    "# Font configuration\n",
    "# If font_path points to a valid .ttf/.otf, we will use PIL TrueType at font_size_px.\n",
    "# Otherwise we fall back to OpenCV Hershey using hershey_scale.\n",
    "font_path = r\"KGRedHands.ttf\"  # set to None or to a valid TTF path\n",
    "font_size_px = 120  # applies when using TrueType font via PIL\n",
    "hershey_scale = 2.5  # applies when using OpenCV Hershey fonts\n",
    "\n",
    "font_color = (255, 255, 255)\n",
    "font_thickness = 3\n",
    "stroke_color = (0, 0, 0)\n",
    "stroke_thickness = 6\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "# Source audio and timings from previous cells\n",
    "source_audio = song_path  # from earlier cell\n",
    "word_timings_in = word_timings  # from earlier cell (list of dicts: word,start,end)\n",
    "\n",
    "# Video temp paths\n",
    "temp_video_path = output_dir / \"lyrics_black_temp.mp4\"\n",
    "final_video_path = output_dir / \"lyrics_black_with_audio.mp4\"\n",
    "\n",
    "# Prepare VideoWriter (H264 via mp4v/avc1 depends on platform; fallback to MJPG)\n",
    "fourcc = cv2.VideoWriter.fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(temp_video_path), fourcc, video_fps, (width, height))\n",
    "\n",
    "# Decide which font pipeline to use\n",
    "use_pil_font = bool(font_path) and Path(font_path).exists()\n",
    "if not use_pil_font and font_path:\n",
    "    print(f\"Warning: font_path not found: {font_path}. Falling back to OpenCV Hershey font.\")\n",
    "\n",
    "# Helper to draw centered text with optional stroke\n",
    "def draw_centered_text(img, text):\n",
    "    if not use_pil_font:\n",
    "        # OpenCV Hershey path (uses hershey_scale)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_size, _ = cv2.getTextSize(text, font, hershey_scale, font_thickness)\n",
    "        tw, th = text_size\n",
    "        x = (width - tw) // 2\n",
    "        y = (height + th) // 2\n",
    "        if stroke_thickness > 0:\n",
    "            cv2.putText(img, text, (x, y), font, hershey_scale, stroke_color, stroke_thickness, line_type)\n",
    "        cv2.putText(img, text, (x, y), font, hershey_scale, font_color, font_thickness, line_type)\n",
    "    else:\n",
    "        # PIL TrueType path (uses font_size_px)\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        pil_img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        try:\n",
    "            fnt = ImageFont.truetype(font_path, font_size_px)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load TrueType font '{font_path}': {e}. Falling back to Hershey.\")\n",
    "            # Fallback to Hershey immediately\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text_size, _ = cv2.getTextSize(text, font, hershey_scale, font_thickness)\n",
    "            tw, th = text_size\n",
    "            x = (width - tw) // 2\n",
    "            y = (height + th) // 2\n",
    "            if stroke_thickness > 0:\n",
    "                cv2.putText(img, text, (x, y), font, hershey_scale, stroke_color, stroke_thickness, line_type)\n",
    "            cv2.putText(img, text, (x, y), font, hershey_scale, font_color, font_thickness, line_type)\n",
    "            return\n",
    "        # Measure text\n",
    "        bbox = draw.textbbox((0, 0), text, font=fnt)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x = (width - tw) // 2\n",
    "        y = (height - th) // 2\n",
    "        # crude stroke by drawing offset shadows\n",
    "        if stroke_thickness > 0:\n",
    "            r = max(1, stroke_thickness // 2)\n",
    "            for dx, dy in [(-r,0),(r,0),(0,-r),(0,r),(-r,-r),(-r,r),(r,-r),(r,r)]:\n",
    "                draw.text((x+dx, y+dy), text, font=fnt, fill=stroke_color)\n",
    "        draw.text((x, y), text, font=fnt, fill=font_color)\n",
    "        img[:] = np.array(pil_img)\n",
    "\n",
    "# Normalize/clean timings\n",
    "def clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "valid_timings = []\n",
    "for w in word_timings_in:\n",
    "    try:\n",
    "        s = float(w.get('start', 0.0))\n",
    "        e = float(w.get('end', s + 0.25))\n",
    "        if e <= s:\n",
    "            e = s + 0.25\n",
    "        valid_timings.append({'word': str(w.get('word','')), 'start': s, 'end': e})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not valid_timings:\n",
    "    raise RuntimeError(\n",
    "        \"word_timings is empty. Please ensure you have:\\n\"\n",
    "        \"1. Run the word alignment cell that populates 'word_timings'\\n\"\n",
    "        \"2. Verified that the alignment completed successfully\\n\"\n",
    "        f\"Current word_timings length: {len(word_timings_in)}\"\n",
    "    )\n",
    "\n",
    "total_duration = valid_timings[-1]['end']\n",
    "\n",
    "def frame_for_time(t):\n",
    "    # Generate a single frame for time t\n",
    "    # Find the active word whose [start,end) contains t\n",
    "    active = None\n",
    "    for w in valid_timings:\n",
    "        if w['start'] <= t < w['end']:\n",
    "            active = w\n",
    "            break\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    if active:\n",
    "        draw_centered_text(img, active['word'])\n",
    "    return img\n",
    "\n",
    "# Render frames\n",
    "num_frames = int(math.ceil(total_duration * video_fps))\n",
    "for i in range(num_frames):\n",
    "    t = i / video_fps\n",
    "    frame = frame_for_time(t)\n",
    "    writer.write(frame)\n",
    "\n",
    "writer.release()\n",
    "print(f\"Wrote silent video: {temp_video_path}\")\n",
    "\n",
    "# Mux audio using ffmpeg\n",
    "# - Copy video stream, re-encode audio to AAC for compatibility\n",
    "# - Shorten to the shorter of video/audio so they end together\n",
    "ffmpeg_cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",\n",
    "    \"-i\", str(temp_video_path),\n",
    "    \"-i\", str(source_audio),\n",
    "    \"-c:v\", \"copy\",\n",
    "    \"-c:a\", \"aac\",\n",
    "    \"-shortest\",\n",
    "    str(final_video_path)\n",
    "]\n",
    "print(\"Running:\", \" \".join(ffmpeg_cmd))\n",
    "try:\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "    print(f\"Wrote final video with audio: {final_video_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ffmpeg not found on PATH. Please install FFmpeg or add it to PATH.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-whisperx-py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
